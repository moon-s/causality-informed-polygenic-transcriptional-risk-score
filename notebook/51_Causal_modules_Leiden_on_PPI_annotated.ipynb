{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ea7336",
   "metadata": {},
   "source": [
    "# 51 Causal modules Leiden on PPI\n",
    "\n",
    "**Origin:** `5_1_causal_module_Leiden_ppi.ipynb`  \n",
    "**Annotated on:** 2025-10-13 06:45\n",
    "\n",
    "**High-level objective:**  \n",
    "- Detect causal communities on the PPI using Leiden/CPM or RB; grid-search resolution; tag modules enriched for MR-significant genes.\n",
    "\n",
    "**Notes:**  \n",
    "- These comments are language-agnostic and focus on intent, inputs, and outputs.  \n",
    "- Adjust hard-coded paths if needed; prefer `/results_*` for derived artifacts.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fe7d3",
   "metadata": {},
   "source": [
    "**Step 1:** Load network or tabular inputs (PPI/GraphML/TSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82adb955-7754-464e-b31e-4ae75d0ff006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned graph: |V|=16,201 |E|=236,930\n",
      "Causal genes in pruned graph: 810 (of 969)\n"
     ]
    }
   ],
   "source": [
    "#below is code for pruning ppi network by top 1% of hubnodes. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import hypergeom\n",
    "import community as community_louvain  # Louvain\n",
    "import os\n",
    "\n",
    "# Load PPI\n",
    "ppi_edges = pd.read_csv('/mnt/f/10_osteo_MR/datasets/ppi/ppi_all_nonduplicate.tsv' ) # ,\n",
    "                       #  sep='\\t', header=None)\n",
    "ppi_edges.columns = ['inx', 'gene_u','gene_v']\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(ppi_edges[['gene_u','gene_v']].itertuples(index=False, name=None))\n",
    "\n",
    "# Load causal genes\n",
    "mr_df = pd.read_csv('/mnt/f/10_osteo_MR/results_mr_ptrs/PTRS/bulk_crossmodal_meta_beta.tsv', sep='\\t')\n",
    "causal_genes = set(mr_df['gene'])\n",
    "\n",
    "outdir = \"/mnt/f/10_osteo_MR/results_network/\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# ==== Step 1. Exclude top 1% hubs ====\n",
    "N = G.number_of_nodes()\n",
    "top_k = int(N * 0.01)\n",
    "deg_sorted = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "hub_nodes = {n for n,_ in deg_sorted[:top_k]}\n",
    "\n",
    "# without pruning\n",
    "G_pruned = G.copy()\n",
    "\n",
    "\n",
    "# Keep causal genes present in the pruned graph\n",
    "causal_genes_in = causal_genes & set(G_pruned.nodes())\n",
    "total_nodes = G_pruned.number_of_nodes()\n",
    "total_causal = len(causal_genes_in)\n",
    "\n",
    "print(f\"Pruned graph: |V|={total_nodes:,} |E|={G_pruned.number_of_edges():,}\")\n",
    "print(f\"Causal genes in pruned graph: {total_causal:,} (of {len(causal_genes):,})\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b49093",
   "metadata": {},
   "source": [
    "**Step 2:** Community detection / resolution sweep on the PPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313c78f-f23d-48f9-bd97-55be59ef30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- Viz ----------\n",
    "def visualize_sig_modules_network(G_nx, g, clustering, comm_df, causal_genes, beta_map,\n",
    "                                  out_png, out_graphml,\n",
    "                                  q_alpha=0.05, min_size=8, seed=42,\n",
    "                                  max_nodes_to_draw=8000):\n",
    "    import matplotlib.patches as mpatches\n",
    "    sig_ids = set(comm_df.loc[(comm_df['is_significant']) &\n",
    "                              (comm_df['q_value'] < q_alpha) &\n",
    "                              (comm_df['size'] >= min_size), 'community'])\n",
    "    if not sig_ids:\n",
    "        print(\"No significant modules to plot.\")\n",
    "        return\n",
    "    names = np.array(g.vs['name'])\n",
    "    keep_vids = np.unique(np.concatenate([np.array(clustering[c], int) for c in sig_ids]))\n",
    "    keep_genes = set(names[keep_vids])\n",
    "\n",
    "    H = nx.Graph()\n",
    "    gene2cid = {}\n",
    "    for cid in sig_ids:\n",
    "        for vid in clustering[cid]:\n",
    "            gene2cid[names[vid]] = cid\n",
    "    for gene in keep_genes:\n",
    "        b = beta_map.get(gene, np.nan)\n",
    "        H.add_node(gene,\n",
    "                   community=int(gene2cid.get(gene, -1)),\n",
    "                   is_causal=(gene in causal_genes),\n",
    "                   beta=b,\n",
    "                   beta_abs=(abs(b) if not np.isnan(b) else 0.0))\n",
    "    for u,v in G_nx.edges():\n",
    "        if u in keep_genes and v in keep_genes:\n",
    "            H.add_edge(u,v)\n",
    "\n",
    "    if H.number_of_nodes() > max_nodes_to_draw:\n",
    "        keep = []\n",
    "        for cid in sig_ids:\n",
    "            members = [n for n,d in H.nodes(data=True) if d.get(\"community\")==cid]\n",
    "            members = sorted(members, key=lambda x: H.degree(x), reverse=True)\n",
    "            keep += members[:max(50, int(0.2*len(members)))]\n",
    "        H = H.subgraph(keep).copy()\n",
    "\n",
    "    pos = nx.spring_layout(H, seed=seed)\n",
    "    cids = sorted({H.nodes[n]['community'] for n in H.nodes()})\n",
    "    cmap = plt.get_cmap(\"Set2\"); cid2idx = {c:i for i,c in enumerate(cids)}\n",
    "    def size_for(n):\n",
    "        babs = H.nodes[n].get('beta_abs', 0.0); return 60 + 240*min(babs,3.0)/3.0\n",
    "\n",
    "    causal_nodes = [n for n in H.nodes() if H.nodes[n].get('is_causal', False)]\n",
    "    noncausal_nodes = [n for n in H.nodes() if not H.nodes[n].get('is_causal', False)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,10), dpi=150)\n",
    "    nx.draw_networkx_edges(H, pos, ax=ax, width=0.3, alpha=0.35, edge_color=\"#999\")\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos, nodelist=noncausal_nodes,\n",
    "        node_size=[size_for(n) for n in noncausal_nodes],\n",
    "        node_color=[cmap(cid2idx[H.nodes[n]['community']]%20) for n in noncausal_nodes],\n",
    "        linewidths=0.2, edgecolors=\"none\", alpha=0.95, ax=ax\n",
    "    )\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos, nodelist=causal_nodes,\n",
    "        node_size=[size_for(n) for n in causal_nodes],\n",
    "        node_color=[cmap(cid2idx[H.nodes[n]['community']]%20) for n in causal_nodes],\n",
    "        linewidths=1.6, edgecolors=\"black\", alpha=0.98, ax=ax\n",
    "    )\n",
    "    handles = [mpatches.Patch(color=cmap(cid2idx[c]%20), label=f\"Module {c}\") for c in cids[:10]]\n",
    "    if handles: ax.legend(handles=handles, title=\"Significant modules\", loc=\"upper right\", frameon=True)\n",
    "    ax.set_title(\"Significant modules (Leiden/CPM)\\nCausal genes bolded\")\n",
    "    ax.set_axis_off(); fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=300); plt.close(fig)\n",
    "    nx.write_graphml(H, out_graphml)\n",
    "    print(f\"Saved: {out_png}\\nSaved: {out_graphml}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd0dc0",
   "metadata": {},
   "source": [
    "**Step 3:** Load network or tabular inputs (PPI/GraphML/TSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c95a6a0-16dc-4e39-9d52-d56511bbdbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full PPI: V=16201 E=236930\n",
      "[OK] Wrote grid results: /mnt/f/10_osteo_MR/results_network/grid_leiden_A2_union_enrichment.csv\n",
      "[OK] Wrote best param set: /mnt/f/10_osteo_MR/results_network/grid_leiden_A2_best.json\n",
      "Best (scheme, alpha, gamma): ('A2', 8.0, 0.00336)\n",
      "Best aggregated_p = 2.8157112461777197e-48 | coverage_rate = 0.31851851851851853 | precision = 0.12451737451737452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "# Grid over (alpha, gamma) for A2 weights on FULL PPI\n",
    "# =========================\n",
    "import os, json, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hypergeom, chi2\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------ Paths ------------\n",
    "PPI_PATH = \"/mnt/f/10_osteo_MR/datasets/ppi/ppi_all_nonduplicate.tsv\"\n",
    "MR_PATH  = \"/mnt/f/10_osteo_MR/results_mr_ptrs/PTRS/bulk_crossmodal_meta_beta.tsv\"\n",
    "OUTDIR   = \"/mnt/f/10_osteo_MR/results_network/\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ------------ Helpers ------------\n",
    "def bh_fdr(p):\n",
    "    p = np.asarray(p, float)\n",
    "    if p.size == 0:\n",
    "        return p\n",
    "    idx = np.argsort(p)\n",
    "    ranks = np.empty_like(idx)\n",
    "    ranks[idx] = np.arange(1, p.size + 1)\n",
    "    q = p * p.size / ranks\n",
    "    q_sorted = np.minimum.accumulate(q[idx][::-1])[::-1]\n",
    "    out = np.empty_like(q)\n",
    "    out[idx] = np.minimum(q_sorted, 1.0)\n",
    "    return out\n",
    "\n",
    "def load_ppi_and_mr(ppi_path, mr_path):\n",
    "    # PPI -> NetworkX\n",
    "    ppi = pd.read_csv(ppi_path)\n",
    "    ppi.columns = ['inx','gene_u','gene_v']\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(ppi[['gene_u','gene_v']].itertuples(index=False, name=None))\n",
    "\n",
    "    # MR causal list with beta\n",
    "    mr = pd.read_csv(mr_path, sep='\\t')[['gene','meta_beta_common']].dropna()\n",
    "    mr['gene'] = mr['gene'].astype(str)\n",
    "    mr_agg = mr.groupby('gene', as_index=False)['meta_beta_common'].mean()\n",
    "    causal = set(mr_agg['gene'])\n",
    "    beta_map = dict(zip(mr_agg['gene'], mr_agg['meta_beta_common']))\n",
    "    return G, causal, beta_map\n",
    "\n",
    "def nx_to_igraph(G, weights=None):\n",
    "    nodes = list(G.nodes())\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "    edges = [(idx[u], idx[v]) for u,v in G.edges()]\n",
    "    g = ig.Graph(n=len(nodes), edges=edges, directed=False)\n",
    "    g.vs['name'] = nodes\n",
    "    if weights is not None:\n",
    "        g.es['weight'] = list(weights)  # aligned to edges order above\n",
    "    return g, nodes\n",
    "\n",
    "# ---------- β-aware weights (A1 / A2) ----------\n",
    "# A1: w = 1 + alpha*(|βu|+|βv|)\n",
    "# A2: w = 1 + alpha*(|βu|+|βv|)*(1.0 if same sign else lambda_diff)\n",
    "def build_weights(G, beta_map, scheme=\"A1\", alpha=0.5, lambda_diff=0.5, beta_clip=3.0,\n",
    "                  normalize_mean1=True, ensure_baseline=True, wmax=None):\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    def b(n):\n",
    "        x = beta_map.get(n, 0.0)\n",
    "        if x is None or np.isnan(x):\n",
    "            x = 0.0\n",
    "        s = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "        return s * min(abs(x), beta_clip)\n",
    "\n",
    "    w = []\n",
    "    for u, v in edges:\n",
    "        bu, bv = b(u), b(v)\n",
    "        mu, mv = abs(bu), abs(bv)\n",
    "        same = (bu == 0 and bv == 0) or (bu >= 0 and bv >= 0) or (bu <= 0 and bv <= 0)\n",
    "        if scheme == \"A1\":\n",
    "            w_uv = 1.0 + alpha * (mu + mv)\n",
    "        elif scheme == \"A2\":\n",
    "            w_uv = 1.0 + alpha * (mu + mv) * (1.0 if same else lambda_diff)\n",
    "        else:\n",
    "            raise ValueError(\"scheme must be 'A1' or 'A2'\")\n",
    "        w.append(max(w_uv, 1e-9))\n",
    "\n",
    "    w = np.array(w, float)\n",
    "    if normalize_mean1 and w.mean() > 0:\n",
    "        w = w / w.mean()\n",
    "    if ensure_baseline:\n",
    "        # enforce: edges touching any nonzero beta > 1, both-zero edges = 1\n",
    "        beta_sig = np.array([(abs(beta_map.get(u, 0.0)) > 0) or (abs(beta_map.get(v, 0.0)) > 0)\n",
    "                             for u, v in edges])\n",
    "        w[~beta_sig] = 1.0\n",
    "        w[ beta_sig] = np.maximum(w[beta_sig], 1.0 + 1e-3)\n",
    "    if wmax is not None:\n",
    "        w = np.minimum(w, float(wmax))\n",
    "    return w\n",
    "\n",
    "# ---------- Enrichment tests ----------\n",
    "def enrich_partition(g, clustering, causal_set, min_size=5, fdr_alpha=0.05):\n",
    "    rows = []\n",
    "    N = g.vcount()\n",
    "    names = np.array(g.vs['name'])\n",
    "    # Universe causal count (over graph vertices)\n",
    "    K = np.isin(names, list(causal_set)).sum()\n",
    "\n",
    "    for cid, members in enumerate(clustering):\n",
    "        size = len(members)\n",
    "        if size < min_size:\n",
    "            continue\n",
    "        genes = names[members]\n",
    "        k = np.isin(genes, list(causal_set)).sum()\n",
    "        # Right-tail hypergeometric: P[X >= k]\n",
    "        p = hypergeom.sf(k - 1, N, K, size)\n",
    "        rows.append(dict(community=cid, size=size, causal_in_comm=k, p_value=p))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        df['q_value'] = []\n",
    "        df['is_significant'] = []\n",
    "        return df\n",
    "\n",
    "    df['q_value'] = bh_fdr(df['p_value'].values)\n",
    "    df['is_significant'] = (df['q_value'] < fdr_alpha) & (df['size'] >= min_size)\n",
    "    #df['is_significant'] =  (df['causal_in_comm'] >= 0 ) #  min_size)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def union_enrichment(comm_df, g, clustering, causal_set):\n",
    "    \"\"\"\n",
    "    Aggregate across significant modules:\n",
    "      - Take the union of vertices from all FDR-significant modules\n",
    "      - Compute a single hypergeometric P for (#causal in union) vs (union size)\n",
    "    Returns dict with counts and p-value. If no significant modules, returns zeros and p=1.\n",
    "    \"\"\"\n",
    "    names = np.array(g.vs['name'])\n",
    "    N = g.vcount()\n",
    "    K = np.isin(names, list(causal_set)).sum()\n",
    "\n",
    "    sig = comm_df[comm_df['is_significant']]\n",
    "    if sig.empty:\n",
    "        return dict(union_nodes=0, union_causal=0, universe_N=N, universe_K=K,\n",
    "                    aggregated_p=1.0)\n",
    "\n",
    "    sig_ids = sig['community'].tolist()\n",
    "    union_vids = set()\n",
    "    for cid in sig_ids:\n",
    "        union_vids.update(clustering[cid])\n",
    "    union_vids = np.array(sorted(list(union_vids)), dtype=int)\n",
    "\n",
    "    union_nodes = int(union_vids.size)\n",
    "    union_genes = names[union_vids]\n",
    "    union_causal = int(np.isin(union_genes, list(causal_set)).sum())\n",
    "\n",
    "    # One-shot enrichment of the union\n",
    "    aggregated_p = float(hypergeom.sf(union_causal - 1, N, K, union_nodes))\n",
    "\n",
    "    return dict(union_nodes=union_nodes,\n",
    "                union_causal=union_causal,\n",
    "                universe_N=N,\n",
    "                universe_K=K,\n",
    "                aggregated_p=aggregated_p)\n",
    "\n",
    "def fishers_method(pvals):\n",
    "    \"\"\"\n",
    "    Fisher's combined probability test for a list of independent p-values.\n",
    "    Useful for diagnostics; not used for ranking unless you want to.\n",
    "    \"\"\"\n",
    "    pvals = [p for p in pvals if p > 0 and np.isfinite(p)]\n",
    "    if len(pvals) == 0:\n",
    "        return 1.0\n",
    "    stat = -2.0 * np.sum(np.log(pvals))\n",
    "    df = 2 * len(pvals)\n",
    "    return float(chi2.sf(stat, df))\n",
    "\n",
    "def coverage_metrics(comm_df, g, clustering, causal_set):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      num_sig: # significant modules\n",
    "      covered_causal: # unique causal genes in significant modules\n",
    "      covered_nodes:  # total nodes (causal + non-causal) in significant modules\n",
    "    \"\"\"\n",
    "    sig = comm_df[comm_df['is_significant']]\n",
    "    num_sig = int(sig.shape[0])\n",
    "    if num_sig == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    names = np.array(g.vs['name'])\n",
    "    sig_ids = set(sig['community'].tolist())\n",
    "\n",
    "    keep_vids = []\n",
    "    for cid in sig_ids:\n",
    "        keep_vids.extend(list(clustering[cid]))\n",
    "    keep_vids = np.unique(np.array(keep_vids, dtype=int))\n",
    "\n",
    "    covered_nodes = int(keep_vids.size)\n",
    "    genes = names[keep_vids]\n",
    "    covered_causal = int(np.isin(genes, list(causal_set)).sum())\n",
    "\n",
    "    return num_sig, covered_causal, covered_nodes\n",
    "\n",
    "# ------------ Load data (FULL PPI) ------------\n",
    "G_full, causal_genes, beta_map = load_ppi_and_mr(PPI_PATH, MR_PATH)\n",
    "print(f\"Full PPI: V={G_full.number_of_nodes()} E={G_full.number_of_edges()}\")\n",
    "\n",
    "# ------------ Grid spec ------------\n",
    "schemes = [\"A2\"]  # keep as a list for easy extension\n",
    "alphas  = np.round(np.arange(0.5, 10.0 + 1e-9, 0.5), 3).tolist()  # 0.5 interval\n",
    "gammas  = np.round(np.logspace(np.log10(1e-3), np.log10(1e-2), 20), 6).tolist()  # 20 log points\n",
    "\n",
    "min_comm_size = 5\n",
    "fdr_alpha = 0.05\n",
    "seed = 42\n",
    "\n",
    "rows = []\n",
    "kept_partitions = {}   # (scheme, alpha, gamma) -> (g, clustering, comm_df)\n",
    "\n",
    "# ------------ Grid ------------\n",
    "for scheme in schemes:\n",
    "    for alpha in alphas:\n",
    "        # Prebuild weights per alpha\n",
    "        w = build_weights(G_full, beta_map,\n",
    "                          scheme=scheme, alpha=alpha,\n",
    "                          lambda_diff=0.5, beta_clip=3.0,\n",
    "                          normalize_mean1=True, ensure_baseline=True)\n",
    "        g, nodes = nx_to_igraph(G_full, weights=w)\n",
    "\n",
    "        for gamma in gammas:\n",
    "            try:\n",
    "                part = la.find_partition(\n",
    "                    g,\n",
    "                    la.CPMVertexPartition,\n",
    "                    weights='weight',\n",
    "                    resolution_parameter=gamma #,\n",
    "                    #seed=seed\n",
    "                )\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Leiden failed: scheme={scheme}, alpha={alpha}, gamma={gamma}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Per-module enrichment & FDR\n",
    "            comm_df = enrich_partition(g, part, causal_genes,\n",
    "                                       min_size=min_comm_size,\n",
    "                                       fdr_alpha=fdr_alpha)\n",
    "\n",
    "            # Coverage over significant modules\n",
    "            num_sig, covered_causal, covered_nodes = coverage_metrics(comm_df, g, part, causal_genes)\n",
    "\n",
    "            # Aggregated (union) enrichment p-value across all significant modules\n",
    "            agg = union_enrichment(comm_df, g, part, causal_genes)\n",
    "            aggregated_p = agg['aggregated_p']\n",
    "            union_nodes = agg['union_nodes']\n",
    "            union_causal = agg['union_causal']\n",
    "\n",
    "            # Optional: Fisher's method across *module-level* (raw) p-values of significant modules\n",
    "            # Not used for ranking unless you decide to.\n",
    "            fish_p = 1.0\n",
    "            if num_sig > 0:\n",
    "                fish_p = fishers_method(comm_df.loc[comm_df['is_significant'], 'p_value'].tolist())\n",
    "\n",
    "            # Useful ratios\n",
    "            total_causal_in_graph = int(agg['universe_K'])\n",
    "            coverage_rate = (covered_causal / total_causal_in_graph) if total_causal_in_graph > 0 else 0.0\n",
    "            precision = (union_causal / union_nodes) if union_nodes > 0 else 0.0\n",
    "\n",
    "            rows.append(dict(\n",
    "                scheme=scheme,\n",
    "                alpha=float(alpha),\n",
    "                gamma=float(gamma),\n",
    "                n_modules=len(part),\n",
    "                num_sig=num_sig,\n",
    "                covered_causal=covered_causal,    # total causal across significant modules (union)\n",
    "                covered_nodes=covered_nodes,      # total nodes across significant modules (union)\n",
    "                union_causal=union_causal,        # same as covered_causal, but explicit naming\n",
    "                union_nodes=union_nodes,\n",
    "                coverage_rate=coverage_rate,      # union_causal / total causal in graph\n",
    "                precision=precision,              # union_causal / union_nodes\n",
    "                aggregated_p=aggregated_p,        # **PRIMARY selection stat**\n",
    "                fishers_p=fish_p                  # optional diagnostic\n",
    "            ))\n",
    "\n",
    "            # Keep artifacts if you want to inspect later\n",
    "            kept_partitions[(scheme, float(alpha), float(gamma))] = (g, part, comm_df)\n",
    "\n",
    "# ------------ Collate & choose best ------------\n",
    "grid_df = pd.DataFrame(rows).sort_values(['scheme', 'alpha', 'gamma']).reset_index(drop=True)\n",
    "\n",
    "# Rank primarily by aggregated_p (smaller is better), then by coverage_rate (higher), then precision (higher)\n",
    "grid_df['rank_key'] = list(zip(grid_df['aggregated_p'],\n",
    "                               -grid_df['coverage_rate'],\n",
    "                               -grid_df['precision']))\n",
    "\n",
    "grid_df = grid_df.sort_values('rank_key', kind='mergesort').reset_index(drop=True)\n",
    "best_row = grid_df.iloc[0].to_dict()\n",
    "\n",
    "# Save outputs\n",
    "csv_path = os.path.join(OUTDIR, \"grid_leiden_A2_union_enrichment.csv\")\n",
    "grid_df.drop(columns=['rank_key']).to_csv(csv_path, index=False)\n",
    "\n",
    "best_path = os.path.join(OUTDIR, \"grid_leiden_A2_best.json\")\n",
    "with open(best_path, \"w\") as f:\n",
    "    json.dump(best_row, f, indent=2)\n",
    "\n",
    "print(f\"[OK] Wrote grid results: {csv_path}\")\n",
    "print(f\"[OK] Wrote best param set: {best_path}\")\n",
    "print(\"Best (scheme, alpha, gamma):\", (best_row['scheme'], best_row['alpha'], best_row['gamma']))\n",
    "print(\"Best aggregated_p =\", best_row['aggregated_p'], \n",
    "      \"| coverage_rate =\", best_row['coverage_rate'], \n",
    "      \"| precision =\", best_row['precision'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbd9ce",
   "metadata": {},
   "source": [
    "**Step 4:** Community detection / resolution sweep on the PPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59a175ca-3463-4953-9fe4-9afb309fa9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subnetwork] |V|=2470 |E|=39835 |components|=1 |largest|=2470 |all_connected|=True\n",
      "gene2cid {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "2455\n",
      "Saved: /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/subnet_sig_modules.png\n",
      "Saved: /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/subnet_sig_modules.graphml\n",
      "[OK] Community stats  : /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/subnet_partition_stats.csv\n",
      "[OK] Sig membership   : /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/subnet_sig_membership.tsv\n",
      "[OK] All membership   : /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/subnet_all_membership.tsv\n",
      "[OK] Scores           : /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/partition_scores.json\n",
      "[OK] Viz (PNG/GraphML): /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/subnet_sig_modules.png / /mnt/f/10_osteo_MR/results_network/subnet_A2_a6_g0.001832981/subnet_sig_modules.graphml\n"
     ]
    }
   ],
   "source": [
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy.stats import hypergeom\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Paths & best params ----------\n",
    "PPI_PATH = \"/mnt/f/10_osteo_MR/datasets/ppi/ppi_all_nonduplicate.tsv\"\n",
    "MR_PATH  = \"/mnt/f/10_osteo_MR/results_mr_ptrs/PTRS/bulk_crossmodal_meta_beta.tsv\"\n",
    "OUTDIR   = \"/mnt/f/10_osteo_MR/results_network/\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "BEST_SCHEME = \"A2\"\n",
    "BEST_ALPHA  = 6.0\n",
    "BEST_GAMMA  = 0.001832981\n",
    "MIN_COMM_SIZE = 5\n",
    "FDR_ALPHA = 0.05\n",
    "SEED = 42\n",
    "\n",
    "# === Reuse your helpers: bh_fdr, load_ppi_and_mr, nx_to_igraph, build_weights, enrich_partition from previous cell ===\n",
    "# If not already in scope, uncomment/paste those helpers here.\n",
    "\n",
    "\n",
    "# ---------- Enrichment tests ----------\n",
    "def enrich_partition_min(g, clustering, causal_set, min_size=5, fdr_alpha=0.05):\n",
    "    rows = []\n",
    "    N = g.vcount()\n",
    "    names = np.array(g.vs['name'])\n",
    "    # Universe causal count (over graph vertices)\n",
    "    K = np.isin(names, list(causal_set)).sum()\n",
    "\n",
    "    for cid, members in enumerate(clustering):\n",
    "        size = len(members)\n",
    "        if size < min_size:\n",
    "            continue\n",
    "        genes = names[members]\n",
    "        k = np.isin(genes, list(causal_set)).sum()\n",
    "        # Right-tail hypergeometric: P[X >= k]\n",
    "        p = hypergeom.sf(k - 1, N, K, size)\n",
    "        rows.append(dict(community=cid, size=size, causal_in_comm=k, p_value=p))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        df['q_value'] = []\n",
    "        df['is_significant'] = []\n",
    "        return df\n",
    "\n",
    "    df['q_value'] = bh_fdr(df['p_value'].values)\n",
    "    #df['is_significant'] = (df['q_value'] < fdr_alpha) & (df['size'] >= min_size)\n",
    "    df['is_significant'] =  (df['causal_in_comm'] > 0 ) #  min_size)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- Utility: collect union of sig modules ----------\n",
    "def union_vids_from_sig(comm_df, clustering):\n",
    "    sig_ids = comm_df.loc[comm_df['is_significant'], 'community'].tolist()\n",
    "    union_vids = set()\n",
    "    for cid in sig_ids:\n",
    "        union_vids.update(clustering[cid])\n",
    "    return np.array(sorted(list(union_vids)), dtype=int)\n",
    "\n",
    "# ---------- Visual (same signature as you shared; kept intact) ----------\n",
    "def visualize_sig_modules_network(G_nx, g, clustering, comm_df, causal_genes, beta_map,\n",
    "                                  out_png, out_graphml,\n",
    "                                  q_alpha=0.05, min_size=8, seed=42,\n",
    "                                  max_nodes_to_draw=8000):\n",
    "    import matplotlib.patches as mpatches\n",
    "    sig_ids = set(comm_df.loc[(comm_df['is_significant'])    &\n",
    "                              # (comm_df['q_value'] < q_alpha) &\n",
    "                               (comm_df['size'] >= min_size), 'community'])\n",
    "    \n",
    "    #print( comm_df ) \n",
    "    \n",
    "    if not sig_ids:\n",
    "        print(\"No significant modules to plot.\")\n",
    "        return\n",
    "    \n",
    "    names = np.array(g.vs['name'])\n",
    "    keep_vids = np.unique(np.concatenate([np.array(clustering[c], int) for c in sig_ids]))\n",
    "\n",
    "    keep_genes = set(names[keep_vids])\n",
    "\n",
    "    H = nx.Graph()\n",
    "    gene2cid = {}\n",
    "    for cid in sig_ids:\n",
    "        for vid in clustering[cid]:\n",
    "            gene2cid[names[vid]] = cid\n",
    "\n",
    "    print( 'gene2cid', set( gene2cid.values()  )) \n",
    "    \n",
    "    for gene in keep_genes:\n",
    "        b = beta_map.get(gene, np.nan)\n",
    "        H.add_node(gene,\n",
    "                   community=int(gene2cid.get(gene, -1)),\n",
    "                   is_causal=(gene in causal_genes),\n",
    "                   beta=b,\n",
    "                   beta_abs=(abs(b) if not np.isnan(b) else 0.0))\n",
    "        #if int(gene2cid.get(gene, -1)) in [ 10, 11 ] :\n",
    "        #    print( gene, int(gene2cid.get(gene, -1)) )\n",
    "    \n",
    "    for u,v in G_nx.edges():\n",
    "        if u in keep_genes and v in keep_genes:\n",
    "            H.add_edge(u,v)\n",
    "\n",
    "    print( H.number_of_nodes ())\n",
    "    \n",
    "    if H.number_of_nodes() > max_nodes_to_draw:\n",
    "        keep = []\n",
    "        for cid in sig_ids:\n",
    "            members = [n for n,d in H.nodes(data=True) if d.get(\"community\")==cid]\n",
    "            members = sorted(members, key=lambda x: H.degree(x), reverse=True)\n",
    "            \n",
    "            keep += members[:max(50, int(0.2*len(members)))]\n",
    "        H = H.subgraph(keep).copy()\n",
    "\n",
    "    pos = nx.spring_layout(H, seed=seed)\n",
    "    cids = sorted({H.nodes[n]['community'] for n in H.nodes()})\n",
    "\n",
    "    cmap = plt.get_cmap(\"Set3\"); cid2idx = {c:i for i,c in enumerate(cids)}\n",
    "    \n",
    "    def size_for(n):\n",
    "        babs = H.nodes[n].get('beta_abs', 0.0); return 60 + 240*min(babs,3.0)/3.0\n",
    "\n",
    "    causal_nodes = [n for n in H.nodes() if H.nodes[n].get('is_causal', False)]\n",
    "    noncausal_nodes = [n for n in H.nodes() if not H.nodes[n].get('is_causal', False)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,10), dpi=150)\n",
    "    nx.draw_networkx_edges(H, pos, ax=ax, width=0.3, alpha=0.35, edge_color=\"#999\")\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos, nodelist=noncausal_nodes,\n",
    "        node_size=[size_for(n) for n in noncausal_nodes],\n",
    "        node_color=[cmap(cid2idx[H.nodes[n]['community']]%20) for n in noncausal_nodes],\n",
    "        linewidths=0.2, edgecolors=\"none\", alpha=0.95, ax=ax\n",
    "    )\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos, nodelist=causal_nodes,\n",
    "        node_size=[size_for(n) for n in causal_nodes],\n",
    "        node_color=[cmap(cid2idx[H.nodes[n]['community']]%20) for n in causal_nodes],\n",
    "        linewidths=1.6, edgecolors=\"black\", alpha=0.98, ax=ax\n",
    "    )\n",
    "    handles = [mpatches.Patch(color=cmap(cid2idx[c]%20), label=f\"Module {c}\") for c in cids[:10]]\n",
    "    if handles: ax.legend(handles=handles, title=\"Significant modules\", loc=\"upper right\", frameon=True)\n",
    "    ax.set_title(\"Significant modules (Leiden/CPM)\\nCausal genes bolded\")\n",
    "    ax.set_axis_off(); fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=300); plt.close(fig)\n",
    "    nx.write_graphml(H, out_graphml)\n",
    "    print(f\"Saved: {out_png}\\nSaved: {out_graphml}\")\n",
    "\n",
    "# ---------- Main refine procedure ----------\n",
    "def refine_on_best_params():\n",
    "    # 1) Load full graph + MR\n",
    "    G_full, causal_genes, beta_map = load_ppi_and_mr(PPI_PATH, MR_PATH)\n",
    "\n",
    "    # 2) Best-params partition on FULL graph (to reconstruct the sig-union)\n",
    "    w_full = build_weights(\n",
    "        G_full, beta_map,\n",
    "        scheme=BEST_SCHEME, alpha=BEST_ALPHA,\n",
    "        lambda_diff=0.5, beta_clip=3.0,\n",
    "        normalize_mean1=True, ensure_baseline=True\n",
    "    )\n",
    "    g_full, _ = nx_to_igraph(G_full, weights=w_full)\n",
    "\n",
    "    base_part = la.find_partition(\n",
    "        g_full,\n",
    "        la.CPMVertexPartition,\n",
    "        weights=\"weight\",\n",
    "        resolution_parameter=BEST_GAMMA,\n",
    "        seed=SEED\n",
    "    )\n",
    "    base_comm = enrich_partition(\n",
    "        g_full, base_part, causal_genes,\n",
    "        min_size=MIN_COMM_SIZE, fdr_alpha=FDR_ALPHA\n",
    "    )\n",
    "\n",
    "    # 3) Build subnetwork = union of vertices from FDR-significant modules\n",
    "    if base_comm.empty or not base_comm['is_significant'].any():\n",
    "        print(\"[WARN] No significant modules with the best params; aborting refine.\")\n",
    "        return\n",
    "\n",
    "    union_vids = union_vids_from_sig(base_comm, base_part)\n",
    "    names_full = np.array(g_full.vs['name'])\n",
    "    union_genes = set(names_full[union_vids])\n",
    "\n",
    "    H_nx = G_full.subgraph(union_genes).copy()\n",
    "    n_nodes = H_nx.number_of_nodes()\n",
    "    n_edges = H_nx.number_of_edges()\n",
    "\n",
    "    # Connectivity check\n",
    "    components = list(nx.connected_components(H_nx))\n",
    "    n_components = len(components)\n",
    "    all_connected = (n_components == 1)\n",
    "    largest_comp_size = max(len(c) for c in components) if n_components > 0 else 0\n",
    "\n",
    "    print(f\"[Subnetwork] |V|={n_nodes} |E|={n_edges} |components|={n_components} \"\n",
    "          f\"|largest|={largest_comp_size} |all_connected|={all_connected}\")\n",
    "\n",
    "    # 4) Run Leiden on subnetwork with default optimiser on CPM partition\n",
    "    #    (we use same gamma on the subgraph)\n",
    "    w_sub = build_weights(\n",
    "        H_nx, beta_map,\n",
    "        scheme=BEST_SCHEME, alpha=BEST_ALPHA,\n",
    "        lambda_diff=0.5, beta_clip=3.0,\n",
    "        normalize_mean1=True, ensure_baseline=True\n",
    "    )\n",
    "    h_ig, h_nodes = nx_to_igraph(H_nx, weights=w_sub)\n",
    "\n",
    "    # Initialise CPM partition and optimise with la.Optimiser()\n",
    "    part = la.RBConfigurationVertexPartition(\n",
    "        h_ig, weights=\"weight\" # ,\n",
    "        #resolution_parameter=BEST_GAMMA\n",
    "    )\n",
    "\n",
    "    # part = la.find_partition(h_ig, la.CPMVertexPartition, weights='weight'  )\n",
    "    \n",
    "    opt = la.Optimiser()\n",
    "    # Optimise until convergence\n",
    "    improved = True\n",
    "    #while improved:\n",
    "    improved = opt.optimise_partition( part  , n_iterations=100)\n",
    "\n",
    "\n",
    "    # 5) Stats for the subnetwork partition\n",
    "    comm_df = enrich_partition_min(\n",
    "        h_ig, part, causal_genes,\n",
    "        min_size=MIN_COMM_SIZE, fdr_alpha=FDR_ALPHA\n",
    "    ).sort_values(['is_significant','q_value','size'], ascending=[False, True, False]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Membership table\n",
    "    membership = np.array(part.membership, dtype=int)\n",
    "    genes = np.array(h_ig.vs['name'])\n",
    "    mem_df = pd.DataFrame({'gene': genes, 'community': membership})\n",
    "\n",
    "    sig_ids = set(comm_df.loc[ comm_df['is_significant'] , 'community'].tolist()) # \n",
    "    sig_membership_df = mem_df[mem_df['community'].isin(sig_ids)].copy()\n",
    "\n",
    "\n",
    "    # 6) Modularity (igraph) and CPM \"quality\" (stability-like)\n",
    "    # Modularity is a different objective, but useful as a descriptive metric.\n",
    "    try:\n",
    "        modularity = h_ig.modularity(membership, weights=h_ig.es['weight'])\n",
    "    except Exception as e:\n",
    "        modularity = np.nan\n",
    "        warnings.warn(f\"igraph modularity failed: {e}\")\n",
    "\n",
    "    # CPM quality (the optimised objective)\n",
    "    cpm_quality = float(part.quality())\n",
    "\n",
    "    # 7) Save outputs\n",
    "    tag = f\"subnet_A2_a{int(BEST_ALPHA)}_g{BEST_GAMMA:.9f}\"\n",
    "    subdir = os.path.join(OUTDIR, tag)\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "    # Connectivity summary\n",
    "    with open(os.path.join(subdir, \"connectivity.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"nodes\": n_nodes,\n",
    "            \"edges\": n_edges,\n",
    "            \"n_components\": n_components,\n",
    "            \"largest_component_size\": largest_comp_size,\n",
    "            \"all_connected\": bool(all_connected)\n",
    "        }, f, indent=2)\n",
    "\n",
    "    comm_df_path = os.path.join(subdir, \"subnet_partition_stats.csv\")\n",
    "    comm_df.to_csv(comm_df_path, index=False)\n",
    "\n",
    "    sig_mem_path = os.path.join(subdir, \"subnet_sig_membership.tsv\")\n",
    "    sig_membership_df.to_csv(sig_mem_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    all_mem_path = os.path.join(subdir, \"subnet_all_membership.tsv\")\n",
    "    mem_df.to_csv(all_mem_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    with open(os.path.join(subdir, \"partition_scores.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"modularity\": modularity,\n",
    "            \"cpm_quality\": cpm_quality,\n",
    "            \"gamma\": BEST_GAMMA,\n",
    "            \"alpha\": BEST_ALPHA\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # 8) Visualise significant modules on the subnetwork partition\n",
    "    png_path = os.path.join(subdir, \"subnet_sig_modules.png\")\n",
    "    graphml_path = os.path.join(subdir, \"subnet_sig_modules.graphml\")\n",
    "    visualize_sig_modules_network(\n",
    "        H_nx, h_ig, part, comm_df,\n",
    "        causal_genes, beta_map,\n",
    "        out_png=png_path, out_graphml=graphml_path,\n",
    "        q_alpha=FDR_ALPHA, min_size=MIN_COMM_SIZE, seed=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"[OK] Community stats  : {comm_df_path}\")\n",
    "    print(f\"[OK] Sig membership   : {sig_mem_path}\")\n",
    "    print(f\"[OK] All membership   : {all_mem_path}\")\n",
    "    print(f\"[OK] Scores           : {os.path.join(subdir, 'partition_scores.json')}\")\n",
    "    print(f\"[OK] Viz (PNG/GraphML): {png_path} / {graphml_path}\")\n",
    "\n",
    "# ---- Run ----\n",
    "refine_on_best_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b35d35",
   "metadata": {},
   "source": [
    "**Step 5:** Load network or tabular inputs (PPI/GraphML/TSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb474fb-a792-4d08-8a58-91a2f9e43c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Largest causal subnetwork] |V|=2470 |E|=39835 |components|=1 (by construction)\n",
      "Saved: /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/causal_modules.pdf\n",
      "Saved: /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/causal_modules.graphml\n",
      "[OK] Community stats  : /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/causal_partition_stats.csv\n",
      "[OK] All membership   : /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/all_membership.tsv\n",
      "[OK] Causal membership: /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/causal_membership.tsv\n",
      "[OK] Scores           : /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/partition_scores.json\n",
      "[OK] Viz (PNG/GraphML): /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/causal_modules.pdf / /mnt/f/10_osteo_MR/results_network/largest_causal_subnet_A2_a6_g0.001832981/causal_modules.graphml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy.stats import hypergeom\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------ Paths & best params ------------\n",
    "PPI_PATH = \"/mnt/f/10_osteo_MR/datasets/ppi/ppi_all_nonduplicate.tsv\"\n",
    "MR_PATH  = \"/mnt/f/10_osteo_MR/results_mr_ptrs/PTRS/bulk_crossmodal_meta_beta.tsv\"\n",
    "OUTDIR   = \"/mnt/f/10_osteo_MR/results_network/\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "BEST_SCHEME = \"A2\"\n",
    "BEST_ALPHA  = 6.0\n",
    "BEST_GAMMA  = 0.001832981\n",
    "SEED = 42\n",
    "\n",
    "MIN_COMM_SIZE_FOR_UNION = 5   # used only to FIND the union subnetwork on full graph\n",
    "FDR_ALPHA_FOR_UNION     = 0.05\n",
    "\n",
    "# ------------ Helpers ------------\n",
    "def bh_fdr(p):\n",
    "    p = np.asarray(p, float)\n",
    "    if p.size == 0: return p\n",
    "    idx = np.argsort(p)\n",
    "    ranks = np.empty_like(idx); ranks[idx] = np.arange(1, p.size+1)\n",
    "    q = p * p.size / ranks\n",
    "    q_sorted = np.minimum.accumulate(q[idx][::-1])[::-1]\n",
    "    out = np.empty_like(q); out[idx] = np.minimum(q_sorted, 1.0)\n",
    "    return out\n",
    "\n",
    "def load_ppi_and_mr(ppi_path, mr_path):\n",
    "    ppi = pd.read_csv(ppi_path)\n",
    "    ppi.columns = ['inx','gene_u','gene_v']\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(ppi[['gene_u','gene_v']].itertuples(index=False, name=None))\n",
    "\n",
    "    mr = pd.read_csv(mr_path, sep='\\t')[['gene','meta_beta_common']].dropna()\n",
    "    mr['gene'] = mr['gene'].astype(str)\n",
    "    mr_agg = mr.groupby('gene', as_index=False)['meta_beta_common'].mean()\n",
    "    causal = set(mr_agg['gene'])\n",
    "    beta_map = dict(zip(mr_agg['gene'], mr_agg['meta_beta_common']))\n",
    "    return G, causal, beta_map\n",
    "\n",
    "def nx_to_igraph(G, weights=None):\n",
    "    nodes = list(G.nodes())\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "    edges = [(idx[u], idx[v]) for u,v in G.edges()]\n",
    "    g = ig.Graph(n=len(nodes), edges=edges, directed=False)\n",
    "    g.vs['name'] = nodes\n",
    "    if weights is not None:\n",
    "        g.es['weight'] = list(weights)\n",
    "    return g, nodes\n",
    "\n",
    "# β-aware weights\n",
    "def build_weights(G, beta_map, scheme=\"A2\", alpha=0.5, lambda_diff=0.5, beta_clip=3.0,\n",
    "                  normalize_mean1=True, ensure_baseline=True, wmax=None):\n",
    "    edges = list(G.edges())\n",
    "    def b(n):\n",
    "        x = beta_map.get(n, 0.0)\n",
    "        if x is None or np.isnan(x): x = 0.0\n",
    "        s = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "        return s * min(abs(x), beta_clip)\n",
    "\n",
    "    w = []\n",
    "    for u,v in edges:\n",
    "        bu, bv = b(u), b(v)\n",
    "        mu, mv = abs(bu), abs(bv)\n",
    "        same = (bu == 0 and bv == 0) or (bu >= 0 and bv >= 0) or (bu <= 0 and bv <= 0)\n",
    "        if scheme == \"A1\":\n",
    "            w_uv = 1.0 + alpha*(mu+mv)\n",
    "        elif scheme == \"A2\":\n",
    "            w_uv = 1.0 + alpha*(mu+mv)*(1.0 if same else lambda_diff)\n",
    "        else:\n",
    "            raise ValueError(\"scheme must be 'A1' or 'A2'\")\n",
    "        w.append(max(w_uv, 1e-9))\n",
    "    w = np.array(w, float)\n",
    "    if normalize_mean1 and w.mean() > 0:\n",
    "        w = w / w.mean()\n",
    "    if ensure_baseline:\n",
    "        beta_sig = np.array([(abs(beta_map.get(u,0.0))>0) or (abs(beta_map.get(v,0.0))>0) for u,v in edges])\n",
    "        w[~beta_sig] = 1.0\n",
    "        w[ beta_sig] = np.maximum(w[beta_sig], 1.0 + 1e-3)\n",
    "    if wmax is not None:\n",
    "        w = np.minimum(w, float(wmax))\n",
    "    return w\n",
    "\n",
    "# ONLY used to FIND the union subnetwork on the FULL graph; not used afterward\n",
    "def enrich_partition_for_union(g, clustering, causal_set, min_size=5, fdr_alpha=0.05):\n",
    "    rows = []\n",
    "    N = g.vcount()\n",
    "    names = np.array(g.vs['name'])\n",
    "    K = np.isin(names, list(causal_set)).sum()\n",
    "    for cid, members in enumerate(clustering):\n",
    "        size = len(members)\n",
    "        if size < min_size: continue\n",
    "        genes = names[members]\n",
    "        k = np.isin(genes, list(causal_set)).sum()\n",
    "        p = hypergeom.sf(k-1, N, K, size)\n",
    "        rows.append(dict(community=cid, size=size, causal_in_comm=k, p_value=p))\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        df['q_value']=[]; df['is_significant']=[]\n",
    "        return df\n",
    "    df['q_value'] = bh_fdr(df['p_value'].values)\n",
    "    df['is_significant'] = (df['q_value'] < fdr_alpha) & (df['size'] >= min_size)\n",
    "    return df\n",
    "\n",
    "def union_vids_from_sig(comm_df, clustering):\n",
    "    sig_ids = comm_df.loc[comm_df['is_significant'], 'community'].tolist()\n",
    "    union_vids = set()\n",
    "    for cid in sig_ids:\n",
    "        union_vids.update(clustering[cid])\n",
    "    return np.array(sorted(list(union_vids)), dtype=int)\n",
    "\n",
    "# ---- Visualization: modules with ≥1 causal gene (no p/q used here) ----\n",
    "def visualize_causal_modules_network(G_nx, g, membership, causal_genes, beta_map,\n",
    "                                     out_png, out_graphml,\n",
    "                                     select_cids, seed=42, max_nodes_to_draw=8000):\n",
    "    import matplotlib.patches as mpatches\n",
    "\n",
    "    names = np.array(g.vs['name'])\n",
    "    cid_by_vid = np.array(membership, dtype=int)\n",
    "    cid_by_gene = {names[i]: int(cid_by_vid[i]) for i in range(len(names))}\n",
    "\n",
    "    # Keep only nodes in selected communities\n",
    "    keep_vids = np.array([i for i,c in enumerate(cid_by_vid) if c in select_cids], dtype=int)\n",
    "    keep_genes = set(names[keep_vids])\n",
    "\n",
    "    H = nx.Graph()\n",
    "    for gene in keep_genes:\n",
    "        b = beta_map.get(gene, np.nan)\n",
    "        H.add_node(gene,\n",
    "                   community=cid_by_gene[gene],\n",
    "                   is_causal=(gene in causal_genes),\n",
    "                   beta=b,\n",
    "                   beta_abs=(abs(b) if not np.isnan(b) else 0.0))\n",
    "    for u,v in G_nx.edges():\n",
    "        if u in keep_genes and v in keep_genes:\n",
    "            H.add_edge(u,v)\n",
    "\n",
    "    # Downsample for readability if huge\n",
    "    if H.number_of_nodes() > max_nodes_to_draw:\n",
    "        keep = []\n",
    "        for cid in sorted(select_cids):\n",
    "            members = [n for n,d in H.nodes(data=True) if d.get(\"community\")==cid]\n",
    "            members = sorted(members, key=lambda x: H.degree(x), reverse=True)\n",
    "            keep += members[:max(50, int(0.2*len(members)))]\n",
    "        H = H.subgraph(keep).copy()\n",
    "\n",
    "    if H.number_of_nodes() == 0:\n",
    "        print(\"Nothing to plot (no nodes in selected communities).\")\n",
    "        return\n",
    "\n",
    "    pos = nx.spring_layout(H, seed=seed)\n",
    "    cids = sorted({H.nodes[n]['community'] for n in H.nodes()})\n",
    "    cmap = plt.get_cmap(\"Set3\"); cid2idx = {c:i for i,c in enumerate(cids)}\n",
    "    def size_for(n):\n",
    "        babs = H.nodes[n].get('beta_abs', 0.0); return 60 + 240*min(babs,3.0)/3.0\n",
    "\n",
    "    causal_nodes = [n for n in H.nodes() if H.nodes[n].get('is_causal', False)]\n",
    "    noncausal_nodes = [n for n in H.nodes() if not H.nodes[n].get('is_causal', False)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9,7), dpi=300)\n",
    "    nx.draw_networkx_edges(H, pos, ax=ax, width=0.3, alpha=0.35, edge_color=\"#999\")\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos, nodelist=noncausal_nodes,\n",
    "        node_size=[size_for(n) for n in noncausal_nodes],\n",
    "        node_color=[cmap(cid2idx[H.nodes[n]['community']]%20) for n in noncausal_nodes],\n",
    "        linewidths=0.2, edgecolors=\"none\", alpha=0.95, ax=ax\n",
    "    )\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos, nodelist=causal_nodes,\n",
    "        node_size=[size_for(n) for n in causal_nodes],\n",
    "        node_color=[cmap(cid2idx[H.nodes[n]['community']]%20) for n in causal_nodes],\n",
    "        linewidths=1.6, edgecolors=\"black\", alpha=0.98, ax=ax\n",
    "    )\n",
    "    handles = [mpatches.Patch(color=cmap(cid2idx[c]%20), label=f\"Module {c}\") for c in cids[:12]]\n",
    "    if handles: ax.legend(handles=handles, title=\"Modules (≥5 causal)\", loc=\"upper right\", frameon=True)\n",
    "    ax.set_title(\"Largest causal subnetwork — modules with ≥5 causal gene\")\n",
    "    ax.set_axis_off(); fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=300); plt.close(fig)\n",
    "    nx.write_graphml(H, out_graphml)\n",
    "    print(f\"Saved: {out_png}\\nSaved: {out_graphml}\")\n",
    "\n",
    "# ------------ Main ------------\n",
    "def optimise_on_largest_causal_subnetwork():\n",
    "    # (A) Load full, detect union subnetwork ONCE (uses significance *only to find the union*)\n",
    "    G_full, causal_genes, beta_map = load_ppi_and_mr(PPI_PATH, MR_PATH)\n",
    "\n",
    "    w_full = build_weights(\n",
    "        G_full, beta_map, scheme=BEST_SCHEME, alpha=BEST_ALPHA,\n",
    "        lambda_diff=0.5, beta_clip=3.0, normalize_mean1=True, ensure_baseline=True\n",
    "    )\n",
    "    g_full, _ = nx_to_igraph(G_full, weights=w_full)\n",
    "\n",
    "    # Partition on FULL graph to get the union subnetwork\n",
    "    base_part = la.find_partition(\n",
    "        g_full, la.CPMVertexPartition,\n",
    "        weights=\"weight\", resolution_parameter=BEST_GAMMA, seed=SEED\n",
    "    )\n",
    "    base_comm = enrich_partition_for_union(\n",
    "        g_full, base_part, causal_genes,\n",
    "        min_size=MIN_COMM_SIZE_FOR_UNION, fdr_alpha=FDR_ALPHA_FOR_UNION\n",
    "    )\n",
    "\n",
    "    if base_comm.empty or not base_comm['is_significant'].any():\n",
    "        raise RuntimeError(\"No significant modules found on the full graph with the best params.\")\n",
    "\n",
    "    union_vids = union_vids_from_sig(base_comm, base_part)\n",
    "    names_full = np.array(g_full.vs['name'])\n",
    "    union_genes = set(names_full[union_vids])\n",
    "\n",
    "    H_union = G_full.subgraph(union_genes).copy()\n",
    "\n",
    "    # (B) Choose the LARGEST connected component as the working subnetwork\n",
    "    components = list(nx.connected_components(H_union))\n",
    "    largest_comp = max(components, key=len) if components else set()\n",
    "    H = H_union.subgraph(largest_comp).copy()\n",
    "\n",
    "    n_nodes, n_edges = H.number_of_nodes(), H.number_of_edges()\n",
    "    print(f\"[Largest causal subnetwork] |V|={n_nodes} |E|={n_edges} |components|=1 (by construction)\")\n",
    "\n",
    "    # (C) Leiden on this component with default Optimiser (no significance tests here)\n",
    "    w_sub = build_weights(\n",
    "        H, beta_map, scheme=BEST_SCHEME, alpha=BEST_ALPHA,\n",
    "        lambda_diff=0.5, beta_clip=3.0, normalize_mean1=True, ensure_baseline=True\n",
    "    )\n",
    "    h_ig, h_nodes = nx_to_igraph(H, weights=w_sub)\n",
    "\n",
    "    # CPM partition + optimiser loop\n",
    "    # part = la.CPMVertexPartition(h_ig, weights=\"weight\" ) # , resolution_parameter=BEST_GAMMA)\n",
    "    part = la.RBConfigurationVertexPartition(h_ig, weights=\"weight\" ) # , resolution_parameter=BEST_GAMMA)\n",
    "    \n",
    "    opt = la.Optimiser()\n",
    "    improved = True\n",
    "    #while improved:\n",
    "    improved = opt.optimise_partition(part, n_iterations=1000)\n",
    "\n",
    "    membership = np.array(part.membership, dtype=int)\n",
    "    genes      = np.array(h_ig.vs['name'])\n",
    "\n",
    "    # (D) Summaries WITHOUT p/q: just size and causal_in_comm\n",
    "    df = pd.DataFrame({\"gene\": genes, \"community\": membership})\n",
    "    comm_counts = df.groupby(\"community\")[\"gene\"].count().rename(\"size\").reset_index()\n",
    "    comm_causal = df.assign(is_causal=df[\"gene\"].isin(causal_genes)) \\\n",
    "                    .groupby(\"community\")[\"is_causal\"].sum().astype(int).rename(\"causal_in_comm\").reset_index()\n",
    "    comm_df = comm_counts.merge(comm_causal, on=\"community\")\n",
    "    comm_df = comm_df.sort_values([\"causal_in_comm\",\"size\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "    # Communities with ≥1 causal gene\n",
    "    causal_cids = set(comm_df.loc[comm_df[\"causal_in_comm\"] > 0, \"community\"].tolist())\n",
    "\n",
    "    # (E) Scores: CPM quality, RBConfiguration quality (stability-like), and modularity\n",
    "    cpm_quality = float(part.quality())\n",
    "    try:\n",
    "        rb_part = la.RBConfigurationVertexPartition(h_ig, resolution_parameter=BEST_GAMMA, initial_membership=membership)\n",
    "        rb_quality = float(rb_part.quality())\n",
    "    except Exception as e:\n",
    "        rb_quality = float(\"nan\")\n",
    "        warnings.warn(f\"RBConfiguration quality failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        modularity = h_ig.modularity(membership, weights=h_ig.es['weight'])\n",
    "    except Exception as e:\n",
    "        modularity = float(\"nan\")\n",
    "        warnings.warn(f\"igraph modularity failed: {e}\")\n",
    "\n",
    "    # (F) Save outputs\n",
    "    tag = f\"largest_causal_subnet_A2_a{int(BEST_ALPHA)}_g{BEST_GAMMA:.9f}\"\n",
    "    subdir = os.path.join(OUTDIR, tag)\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "    # Connectivity and size\n",
    "    with open(os.path.join(subdir, \"connectivity.json\"), \"w\") as f:\n",
    "        json.dump({\"nodes\": n_nodes, \"edges\": n_edges, \"n_components\": 1}, f, indent=2)\n",
    "\n",
    "    # Community stats (no p/q)\n",
    "    comm_df_path = os.path.join(subdir, \"causal_partition_stats.csv\")\n",
    "    comm_df.to_csv(comm_df_path, index=False)\n",
    "\n",
    "    # Membership tables\n",
    "    all_mem_path   = os.path.join(subdir, \"all_membership.tsv\")\n",
    "    df.to_csv(all_mem_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    causal_mem_path = os.path.join(subdir, \"causal_membership.tsv\")\n",
    "    df[df[\"community\"].isin(causal_cids)].to_csv(causal_mem_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    # Scores\n",
    "    with open(os.path.join(subdir, \"partition_scores.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"alpha\": BEST_ALPHA,\n",
    "            \"gamma\": BEST_GAMMA,\n",
    "            \"cpm_quality\": cpm_quality,\n",
    "            \"rbconfig_quality\": rb_quality,\n",
    "            \"modularity\": modularity\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # (G) Visualise only modules with ≥1 causal gene\n",
    "    png_path = os.path.join(subdir, \"causal_modules.pdf\")\n",
    "    graphml_path = os.path.join(subdir, \"causal_modules.graphml\")\n",
    "    visualize_causal_modules_network(\n",
    "        H, h_ig, membership, causal_genes, beta_map,\n",
    "        out_png=png_path, out_graphml=graphml_path,\n",
    "        select_cids=causal_cids, seed=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"[OK] Community stats  : {comm_df_path}\")\n",
    "    print(f\"[OK] All membership   : {all_mem_path}\")\n",
    "    print(f\"[OK] Causal membership: {causal_mem_path}\")\n",
    "    print(f\"[OK] Scores           : {os.path.join(subdir, 'partition_scores.json')}\")\n",
    "    print(f\"[OK] Viz (PNG/GraphML): {png_path} / {graphml_path}\")\n",
    "\n",
    "# ---- Run ----\n",
    "optimise_on_largest_causal_subnetwork()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
