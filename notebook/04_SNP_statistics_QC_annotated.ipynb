{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07faf6c8",
   "metadata": {},
   "source": [
    "# 04 SNP statistics QC\n",
    "\n",
    "**Origin:** `0_4_snp_stats.ipynb`  \n",
    "**This annotated version was generated on:** 2025-10-13 06:41\n",
    "\n",
    "**What this notebook does (high level):**  \n",
    "- Compute SNP-level descriptive statistics (MAF, INFO, HWE proxies), counts by annotation, and QC summaries.\n",
    "\n",
    "**How to use:**  \n",
    "1. Review the markdown notes before each code cell.  \n",
    "2. Adjust input/output paths as needed for your environment.  \n",
    "3. Run cell-by-cell to reproduce artifacts for downstream steps.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e07309",
   "metadata": {},
   "source": [
    "**Step 1:** Load tabular data (summary stats / annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea46243-c010-479e-8439-85e97c966a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] RSID→DHS mappings loaded: 32,364,424\n",
      "[run] outcome_osteo\n",
      "[run] exposure_eqtlgen\n",
      "[run] exposure_gtex_wb_eqtl\n",
      "[run] exposure_decode_pqtl\n",
      "[run] exposure_ukbppp_pqtl\n",
      "\n",
      "[done] Wrote overview → /mnt/f/10_osteo_MR/_results_tables/regulatory_variant_stats.overview.csv\n",
      "Per-cohort summaries & tables saved beside it.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Regulatory-variant stats per cohort (fast; RSID join; lymphoid precedence)\n",
    "\n",
    "import os, re, gzip\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Paths ----------\n",
    "mapped_dir  = \"/mnt/f/0.datasets/ens_vcf_dhs/\"  # rsid_in_DHS_chr{n}_{lym|mye}.tsv\n",
    "results_dir = \"/mnt/f/10_osteo_MR/_results_tables/\"\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cohort files (already \"within DHS\" but we re-confirm by RSID join)\n",
    "cohorts = {\n",
    "    \"outcome_osteo\": \"/mnt/f/10_osteo_MR/MR_ready/outcome_osteo_within_DHS.tsv\",\n",
    "    \"exposure_eqtlgen\": \"/mnt/f/10_osteo_MR/MR_ready/exposure_eqtlgen_dhs_index.tsv\",\n",
    "    \"exposure_gtex_wb_eqtl\": \"/mnt/f/10_osteo_MR/MR_ready/exposure_gtex_whole_blood_eqtl_dhs_index.tsv\",\n",
    "    \"exposure_decode_pqtl\": \"/mnt/f/10_osteo_MR/MR_ready/exposure_pqtl_decode_MR_dhs.tsv.gz\",\n",
    "    \"exposure_ukbppp_pqtl\": \"/mnt/f/10_osteo_MR/MR_ready/exposure_ukbppp_pqtl_MR_dhs.tsv.gz\",\n",
    "}\n",
    "\n",
    "# Column hints for each file (so we only read what we need)\n",
    "# - 'SNP' (rsID), 'pval' always read if present\n",
    "# - 'gene' used for DHS-per-gene\n",
    "needed_cols = {\n",
    "    \"outcome_osteo\":       [\"SNP\", \"pval\"],\n",
    "    \"exposure_eqtlgen\":    [\"SNP\", \"pval\", \"gene\"],\n",
    "    \"exposure_gtex_wb_eqtl\":[\"SNP\", \"pval\", \"gene\"],   # 'gene' present per your example\n",
    "    \"exposure_decode_pqtl\":[\"SNP\", \"pval\", \"gene\"],\n",
    "    \"exposure_ukbppp_pqtl\":[\"SNP\", \"pval\", \"gene\"],\n",
    "}\n",
    "\n",
    "# ---------- Parameters ----------\n",
    "PVAL_THRESH = 5e-8  # change if you prefer FDR or dataset-specific cutoffs\n",
    "\n",
    "# ---------- Build RSID -> (identifier, component) with lymphoid precedence ----------\n",
    "rsid2dhs = {}  # rsid -> (identifier:str, component:str)\n",
    "\n",
    "for chr_i in range(1, 23):\n",
    "    # 1) load myeloid/ery first\n",
    "    p_mye = os.path.join(mapped_dir, f\"rsid_in_DHS_chr{chr_i}_mye.tsv\")\n",
    "    if Path(p_mye).exists():\n",
    "        df_m = pd.read_csv(p_mye, sep=r\"\\s+|,\", engine=\"python\",\n",
    "                           usecols=[\"rsid\",\"identifier\",\"component\"],\n",
    "                           dtype={\"rsid\":\"string\",\"identifier\":\"string\",\"component\":\"string\"})\n",
    "        for r, ident, comp in zip(df_m[\"rsid\"], df_m[\"identifier\"], df_m[\"component\"]):\n",
    "            if pd.isna(r): continue\n",
    "            rsid2dhs[str(r)] = (None if pd.isna(ident) else str(ident),\n",
    "                                None if pd.isna(comp)  else str(comp))\n",
    "    # 2) override with lymphoid (precedence)\n",
    "    p_lym = os.path.join(mapped_dir, f\"rsid_in_DHS_chr{chr_i}_lym.tsv\")\n",
    "    if Path(p_lym).exists():\n",
    "        df_l = pd.read_csv(p_lym, sep=r\"\\s+|,\", engine=\"python\",\n",
    "                           usecols=[\"rsid\",\"identifier\",\"component\"],\n",
    "                           dtype={\"rsid\":\"string\",\"identifier\":\"string\",\"component\":\"string\"})\n",
    "        for r, ident, comp in zip(df_l[\"rsid\"], df_l[\"identifier\"], df_l[\"component\"]):\n",
    "            if pd.isna(r): continue\n",
    "            rsid2dhs[str(r)] = (None if pd.isna(ident) else str(ident),\n",
    "                                None if pd.isna(comp)  else str(comp))\n",
    "\n",
    "print(f\"[info] RSID→DHS mappings loaded: {len(rsid2dhs):,}\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def read_table(path, usecols=None):\n",
    "    return pd.read_csv(path, sep=\"\\t\", compression=\"infer\", dtype=\"string\",\n",
    "                       usecols=[c for c in (usecols or []) if c is not None])\n",
    "\n",
    "def stats_for_cohort(name, path):\n",
    "    cols = needed_cols.get(name, [\"SNP\", \"pval\"])\n",
    "    df = read_table(path, cols)\n",
    "    if \"SNP\" not in df.columns:\n",
    "        raise ValueError(f\"{name}: missing 'SNP' column.\")\n",
    "\n",
    "    # Normalize dtypes\n",
    "    df[\"SNP\"]  = df[\"SNP\"].astype(\"string\")\n",
    "    if \"pval\" in df.columns:\n",
    "        # Convert to float; tolerate sci-notation in strings\n",
    "        df[\"pval\"] = pd.to_numeric(df[\"pval\"], errors=\"coerce\")\n",
    "\n",
    "    # Attach DHS identifier/component via RSID map\n",
    "    mapped = df[\"SNP\"].map(rsid2dhs.get)\n",
    "    has_map = mapped.notna()\n",
    "    df_map = pd.DataFrame(mapped.tolist(), index=df.index, columns=[\"identifier\",\"component\"])\n",
    "    df = pd.concat([df, df_map], axis=1)\n",
    "\n",
    "    # Filter to rows with a valid DHS mapping (safety)\n",
    "    df_in = df.loc[has_map].copy()\n",
    "\n",
    "    # ---- Totals / significance (row-level) ----\n",
    "    total_rows_in_dhs = int(len(df_in))\n",
    "    sig_rows_in_dhs   = int((df_in[\"pval\"] < PVAL_THRESH).sum()) if \"pval\" in df_in.columns else None\n",
    "    frac_sig          = (sig_rows_in_dhs / total_rows_in_dhs) if (sig_rows_in_dhs is not None and total_rows_in_dhs>0) else None\n",
    "\n",
    "    # ---- Per-DHS: unique SNPs per DHS identifier ----\n",
    "    # Use unique SNPs to avoid counting duplicate variant-gene associations multiple times.\n",
    "    snps_per_dhs = (df_in.dropna(subset=[\"identifier\"])\n",
    "                       .drop_duplicates(subset=[\"SNP\",\"identifier\"])\n",
    "                       .groupby([\"identifier\",\"component\"], as_index=False)[\"SNP\"]\n",
    "                       .nunique()\n",
    "                       .rename(columns={\"SNP\":\"n_unique_snps\"}))\n",
    "\n",
    "    # ---- Per-gene: unique DHS count per gene (if gene column exists) ----\n",
    "    dhs_per_gene = None\n",
    "    if \"gene\" in df_in.columns:\n",
    "        dhs_per_gene = (df_in.dropna(subset=[\"gene\",\"identifier\"])\n",
    "                           .drop_duplicates(subset=[\"gene\",\"identifier\"])\n",
    "                           .groupby(\"gene\", as_index=False)[\"identifier\"]\n",
    "                           .nunique()\n",
    "                           .rename(columns={\"identifier\":\"n_unique_dhs\"}))\n",
    "\n",
    "    # ---- Unique SNPs total in DHS (optional extra QC) ----\n",
    "    n_unique_snps_in_dhs = int(df_in[\"SNP\"].nunique())\n",
    "\n",
    "    # ---- Save outputs ----\n",
    "    base = os.path.join(results_dir, f\"{name}\")\n",
    "    # summary\n",
    "    summary_rows = [\n",
    "        {\"metric\":\"rows_within_DHS\", \"value\": total_rows_in_dhs},\n",
    "        {\"metric\":\"unique_snps_within_DHS\", \"value\": n_unique_snps_in_dhs},\n",
    "    ]\n",
    "    if sig_rows_in_dhs is not None:\n",
    "        summary_rows += [\n",
    "            {\"metric\":f\"significant_rows_p<{PVAL_THRESH}\", \"value\": sig_rows_in_dhs},\n",
    "            {\"metric\":\"fraction_significant_rows\", \"value\": round(frac_sig, 6) if frac_sig is not None else None},\n",
    "        ]\n",
    "    pd.DataFrame(summary_rows).to_csv(base + \".summary.csv\", index=False)\n",
    "\n",
    "    # per DHS\n",
    "    snps_per_dhs.to_csv(base + \".per_dhs_unique_snp_counts.csv\", index=False)\n",
    "\n",
    "    # per gene (when available)\n",
    "    if dhs_per_gene is not None:\n",
    "        dhs_per_gene.to_csv(base + \".per_gene_unique_dhs_counts.csv\", index=False)\n",
    "\n",
    "    # Return a compact dict for the combined overview\n",
    "    return {\n",
    "        \"cohort\": name,\n",
    "        \"rows_within_DHS\": total_rows_in_dhs,\n",
    "        \"unique_snps_within_DHS\": n_unique_snps_in_dhs,\n",
    "        \"significant_rows_p<{}\".format(PVAL_THRESH): sig_rows_in_dhs,\n",
    "        \"fraction_significant_rows\": (round(frac_sig, 6) if frac_sig is not None else None),\n",
    "        \"per_dhs_file\": os.path.basename(base + \".per_dhs_unique_snp_counts.csv\"),\n",
    "        \"per_gene_file\": os.path.basename(base + \".per_gene_unique_dhs_counts.csv\") if \"gene\" in (dhs_per_gene.columns if dhs_per_gene is not None else []) else None,\n",
    "    }\n",
    "\n",
    "# ---------- Run ----------\n",
    "overview = []\n",
    "for name, path in cohorts.items():\n",
    "    print(f\"[run] {name}\")\n",
    "    overview.append(stats_for_cohort(name, path))\n",
    "\n",
    "overview_df = pd.DataFrame(overview)\n",
    "overview_path = os.path.join(results_dir, \"regulatory_variant_stats.overview.csv\")\n",
    "overview_df.to_csv(overview_path, index=False)\n",
    "print(f\"\\n[done] Wrote overview → {overview_path}\")\n",
    "print(\"Per-cohort summaries & tables saved beside it.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0372158",
   "metadata": {},
   "source": [
    "**Step 2:** Load tabular data (summary stats / annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab90a621-12a8-4d66-af02-cf4ed54da254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               cohort  n_DHS  mean_snps_per_DHS  se_snps_per_DHS mean±se_snps_per_DHS  n_genes  mean_DHS_per_gene  se_DHS_per_gene mean±se_DHS_per_gene\n",
      " exposure_decode_pqtl  72005           1.475634         0.005277        1.476 ± 0.005   4055.0          75.265845         1.980471       75.266 ± 1.980\n",
      "     exposure_eqtlgen 109650           1.392795         0.002752        1.393 ± 0.003  15833.0          23.756521         0.187132       23.757 ± 0.187\n",
      "exposure_gtex_wb_eqtl  52538           1.395580         0.006022        1.396 ± 0.006  10881.0          10.869497         0.168233       10.869 ± 0.168\n",
      " exposure_ukbppp_pqtl  54158           1.468204         0.006670        1.468 ± 0.007   2493.0         107.072603         3.145632      107.073 ± 3.146\n",
      "        outcome_osteo 292131           1.991781         0.002604        1.992 ± 0.003      NaN                NaN              NaN                   NA\n",
      "\n",
      "Saved → /mnt/f/10_osteo_MR/_results_tables/regulatory_variant_stats.mean_se_by_cohort.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Aggregate stats: mean ± s.e. (SNPs per DHS, DHS per gene) per cohort\n",
    "\n",
    "import os, math, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results_dir = \"/mnt/f/10_osteo_MR/_results_tables/\"\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find per-cohort files produced earlier\n",
    "per_dhs_files  = {Path(p).name.split(\".per_dhs_unique_snp_counts.csv\")[0]: p\n",
    "                  for p in glob.glob(os.path.join(results_dir, \"*.per_dhs_unique_snp_counts.csv\"))}\n",
    "per_gene_files = {Path(p).name.split(\".per_gene_unique_dhs_counts.csv\")[0]: p\n",
    "                  for p in glob.glob(os.path.join(results_dir, \"*.per_gene_unique_dhs_counts.csv\"))}\n",
    "\n",
    "cohorts = sorted(set(per_dhs_files) | set(per_gene_files))\n",
    "\n",
    "rows = []\n",
    "for cohort in cohorts:\n",
    "    # --- SNPs per DHS ---\n",
    "    n_dhs = mean_snps = se_snps = None\n",
    "    if cohort in per_dhs_files:\n",
    "        df_dhs = pd.read_csv(per_dhs_files[cohort])\n",
    "        # Expect columns: identifier, component, n_unique_snps\n",
    "        if \"n_unique_snps\" in df_dhs.columns and len(df_dhs) > 0:\n",
    "            n_dhs   = int(len(df_dhs))\n",
    "            mean_snps = float(df_dhs[\"n_unique_snps\"].mean())\n",
    "            se_snps   = float(df_dhs[\"n_unique_snps\"].std(ddof=1) / math.sqrt(n_dhs)) if n_dhs > 1 else np.nan\n",
    "\n",
    "    # --- DHS per gene ---\n",
    "    n_genes = mean_dhs = se_dhs = None\n",
    "    if cohort in per_gene_files:\n",
    "        df_gene = pd.read_csv(per_gene_files[cohort])\n",
    "        # Expect columns: gene, n_unique_dhs\n",
    "        if \"n_unique_dhs\" in df_gene.columns and len(df_gene) > 0:\n",
    "            n_genes = int(len(df_gene))\n",
    "            mean_dhs = float(df_gene[\"n_unique_dhs\"].mean())\n",
    "            se_dhs   = float(df_gene[\"n_unique_dhs\"].std(ddof=1) / math.sqrt(n_genes)) if n_genes > 1 else np.nan\n",
    "\n",
    "    # Pretty “mean ± s.e.” strings (rounded)\n",
    "    def fmt(mean, se, digits=3):\n",
    "        if mean is None or (isinstance(mean, float) and np.isnan(mean)):\n",
    "            return \"NA\"\n",
    "        if se is None or (isinstance(se, float) and np.isnan(se)):\n",
    "            return f\"{mean:.{digits}f} ± NA\"\n",
    "        return f\"{mean:.{digits}f} ± {se:.{digits}f}\"\n",
    "\n",
    "    rows.append({\n",
    "        \"cohort\": cohort,\n",
    "        \"n_DHS\": n_dhs,\n",
    "        \"mean_snps_per_DHS\": None if mean_snps is None else round(mean_snps, 6),\n",
    "        \"se_snps_per_DHS\": None if se_snps is None or np.isnan(se_snps) else round(se_snps, 6),\n",
    "        \"mean±se_snps_per_DHS\": fmt(mean_snps, se_snps),\n",
    "\n",
    "        \"n_genes\": n_genes,\n",
    "        \"mean_DHS_per_gene\": None if mean_dhs is None else round(mean_dhs, 6),\n",
    "        \"se_DHS_per_gene\": None if se_dhs is None or np.isnan(se_dhs) else round(se_dhs, 6),\n",
    "        \"mean±se_DHS_per_gene\": fmt(mean_dhs, se_dhs),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows).sort_values(\"cohort\")\n",
    "\n",
    "out_path = os.path.join(results_dir, \"regulatory_variant_stats.mean_se_by_cohort.csv\")\n",
    "summary_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nSaved → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efdad9",
   "metadata": {},
   "source": [
    "**Step 3:** Load tabular data (summary stats / annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9eb4eb5-1121-4640-b39b-00491001d132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] RSIDs with coordinates: 32,364,424\n",
      "[info] Genes with windows: 41,164\n",
      "[run] exposure_eqtlgen\n",
      "  -> kept 333,980 / 487,985 rows\n",
      "[run] exposure_gtex_wb_eqtl\n",
      "  -> kept 113,984 / 186,199 rows\n",
      "[run] exposure_decode_pqtl\n",
      "  -> kept 55,844 / 675,513 rows\n",
      "[run] exposure_ukbppp_pqtl\n",
      "  -> kept 68,686 / 633,569 rows\n",
      "\n",
      "[done] Saved overview → /mnt/f/10_osteo_MR/_results_tables/exposures_within_gene_window/overview.within_gene±500000_500000.csv\n",
      "Outputs in: /mnt/f/10_osteo_MR/_results_tables/exposures_within_gene_window\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# Subset exposure SNPs to variants inside each row's gene body ±500 bp (RSID→coord + gene coords)\n",
    "\n",
    "import os, re, glob, gzip\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "mapped_dir  = \"/mnt/f/0.datasets/ens_vcf_dhs/\"  # rsid_in_DHS_chr{n}_{lym|mye}.tsv (columns: rsid, POS, component, identifier)\n",
    "gene_path   = \"/mnt/f/10_osteo_MR/datasets/mart_export.txt\"  # Ensembl Biomart export\n",
    "cohorts = {\n",
    "    \"exposure_eqtlgen\":       \"/mnt/f/10_osteo_MR/MR_ready/exposure_eqtlgen_dhs_index.tsv\",\n",
    "    \"exposure_gtex_wb_eqtl\":  \"/mnt/f/10_osteo_MR/MR_ready/exposure_gtex_whole_blood_eqtl_dhs_index.tsv\",\n",
    "    \"exposure_decode_pqtl\":   \"/mnt/f/10_osteo_MR/MR_ready/exposure_pqtl_decode_MR_dhs.tsv.gz\",\n",
    "    \"exposure_ukbppp_pqtl\":   \"/mnt/f/10_osteo_MR/MR_ready/exposure_ukbppp_pqtl_MR_dhs.tsv.gz\",\n",
    "}\n",
    "out_dir = \"/mnt/f/10_osteo_MR/_results_tables/exposures_within_gene_window\"\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- Parameters ----------------\n",
    "UPSTREAM_BP   = 500000   # extend upstream\n",
    "DOWNSTREAM_BP = 500000  # extend downstream\n",
    "CHUNKSIZE = 1_000_000\n",
    "\n",
    "# ---------------- 1) Build RSID → (CHR, POS) with Lymphoid precedence ----------------\n",
    "rsid2coord = {}  # rsid -> (chr_str, pos_int)\n",
    "\n",
    "# Helper: parse chr index from filename (supports numbers, X, Y, MT)\n",
    "chr_pat = re.compile(r\"rsid_in_DHS_chr([0-9XYMT]+)_(lym|mye)\\.tsv$\")\n",
    "\n",
    "# First load mye, then override with lym (precedence)\n",
    "for prio in (\"mye\", \"lym\"):\n",
    "    for p in glob.glob(os.path.join(mapped_dir, f\"rsid_in_DHS_chr*_{prio}.tsv\")):\n",
    "        m = chr_pat.search(os.path.basename(p))\n",
    "        if not m: \n",
    "            continue\n",
    "        chr_str = m.group(1)  # '1'..'22', or 'X','Y','MT'\n",
    "        df = pd.read_csv(p, sep=r\"\\s+|,\", engine=\"python\",\n",
    "                         usecols=[\"rsid\",\"POS\"], dtype={\"rsid\":\"string\",\"POS\":\"Int64\"})\n",
    "        df = df.dropna(subset=[\"rsid\", \"POS\"])\n",
    "        for r, pos in zip(df[\"rsid\"].astype(str), df[\"POS\"].astype(int)):\n",
    "            # Lymphoid overrides myeloid if same RSID appears in both\n",
    "            if prio == \"lym\" or r not in rsid2coord:\n",
    "                rsid2coord[r] = (chr_str, int(pos))\n",
    "\n",
    "print(f\"[info] RSIDs with coordinates: {len(rsid2coord):,}\")\n",
    "\n",
    "# ---------------- 2) Load + collapse gene coordinates; build ±500bp windows ----------------\n",
    "# Expect columns: Gene stable ID, Gene name, Chromosome/scaffold name, Gene start (bp), Gene end (bp)\n",
    "genes = pd.read_csv(gene_path, sep=\"\\t\", dtype=\"string\")\n",
    "genes = genes.rename(columns={\n",
    "    \"Gene stable ID\": \"gene_id\",\n",
    "    \"Gene name\": \"gene\",\n",
    "    \"Chromosome/scaffold name\": \"chr\",\n",
    "    \"Gene start (bp)\": \"start\",\n",
    "    \"Gene end (bp)\": \"end\",\n",
    "})\n",
    "# Clean/convert\n",
    "genes[\"chr\"]   = genes[\"chr\"].astype(str).str.replace(\"^chr\",\"\",regex=True)  # normalize (e.g., '1','X','MT')\n",
    "genes[\"start\"] = pd.to_numeric(genes[\"start\"], errors=\"coerce\")\n",
    "genes[\"end\"]   = pd.to_numeric(genes[\"end\"], errors=\"coerce\")\n",
    "genes = genes.dropna(subset=[\"gene\",\"chr\",\"start\",\"end\"])\n",
    "genes[\"start\"] = genes[\"start\"].astype(int)\n",
    "genes[\"end\"]   = genes[\"end\"].astype(int)\n",
    "genes = genes[genes[\"end\"] >= genes[\"start\"]].copy()\n",
    "\n",
    "# Some genes may appear multiple times (alt loci / transcripts).\n",
    "# Rule: choose the span with the largest length per (gene, chr), then pick the chromosome with the largest span overall.\n",
    "genes[\"span\"] = genes[\"end\"] - genes[\"start\"]\n",
    "largest_per_chr = (genes.sort_values(\"span\", ascending=False)\n",
    "                        .groupby([\"gene\",\"chr\"], as_index=False)\n",
    "                        .first())  # largest span per gene-chr\n",
    "# Now pick the single best chr per gene (largest span)\n",
    "best_per_gene = (largest_per_chr.sort_values(\"span\", ascending=False)\n",
    "                               .groupby(\"gene\", as_index=False)\n",
    "                               .first()[[\"gene\",\"chr\",\"start\",\"end\"]])\n",
    "\n",
    "# Build window\n",
    "best_per_gene[\"win_start\"] = (best_per_gene[\"start\"] - UPSTREAM_BP).clip(lower=1)\n",
    "best_per_gene[\"win_end\"]   = best_per_gene[\"end\"] + DOWNSTREAM_BP\n",
    "\n",
    "# index for fast lookups\n",
    "gene2coord = {g: (c, int(s), int(e)) \n",
    "              for g, c, s, e in zip(best_per_gene[\"gene\"], best_per_gene[\"chr\"],\n",
    "                                     best_per_gene[\"win_start\"], best_per_gene[\"win_end\"])}\n",
    "\n",
    "print(f\"[info] Genes with windows: {len(gene2coord):,}\")\n",
    "\n",
    "# ---------------- 3) Per-cohort filtering ----------------\n",
    "def process_cohort(name: str, path: str):\n",
    "    out_path = os.path.join(out_dir, f\"{name}.within_gene±{UPSTREAM_BP}_{DOWNSTREAM_BP}.tsv.gz\")\n",
    "    # Columns we need\n",
    "    usecols = None  # read all to preserve, but we access SNP + gene (+ pval) by name\n",
    "    written = total = matched_gene = matched_rsid = 0\n",
    "    first = True\n",
    "\n",
    "    def row_mask(df):\n",
    "        nonlocal matched_gene, matched_rsid\n",
    "        # Normalize SNP & gene cols\n",
    "        if \"SNP\" not in df.columns or \"gene\" not in df.columns:\n",
    "            missing = [c for c in (\"SNP\",\"gene\") if c not in df.columns]\n",
    "            raise ValueError(f\"{name}: missing columns: {missing}\")\n",
    "        snp = df[\"SNP\"].astype(\"string\")\n",
    "        gen = df[\"gene\"].astype(\"string\")\n",
    "\n",
    "        # Map rsid to coord\n",
    "        coords = snp.map(rsid2coord.get)  # (chr, pos) or None\n",
    "        has_rsid = coords.notna()\n",
    "        matched_rsid += int(has_rsid.sum())\n",
    "\n",
    "        # Split into two arrays for speed\n",
    "        chr_arr = []\n",
    "        pos_arr = []\n",
    "        for t in coords.fillna(\"\").tolist():\n",
    "            if not t:\n",
    "                chr_arr.append(None); pos_arr.append(None)\n",
    "            else:\n",
    "                c,p = t\n",
    "                chr_arr.append(str(c)); pos_arr.append(int(p))\n",
    "        chr_arr = pd.Series(chr_arr, index=df.index, dtype=\"object\")\n",
    "        pos_arr = pd.Series(pos_arr, index=df.index, dtype=\"float\").astype(\"Int64\")\n",
    "\n",
    "        # Map gene to window\n",
    "        gene_win = gen.map(gene2coord.get)  # (chr, wstart, wend) or None\n",
    "        has_gene = gene_win.notna()\n",
    "        matched_gene += int(has_gene.sum())\n",
    "\n",
    "        gw_chr = []; gw_s = []; gw_e = []\n",
    "        for t in gene_win.fillna(\"\").tolist():\n",
    "            if not t:\n",
    "                gw_chr.append(None); gw_s.append(None); gw_e.append(None)\n",
    "            else:\n",
    "                c,s,e = t\n",
    "                gw_chr.append(str(c)); gw_s.append(int(s)); gw_e.append(int(e))\n",
    "        gw_chr = pd.Series(gw_chr, index=df.index, dtype=\"object\")\n",
    "        gw_s   = pd.Series(gw_s,   index=df.index, dtype=\"float\").astype(\"Int64\")\n",
    "        gw_e   = pd.Series(gw_e,   index=df.index, dtype=\"float\").astype(\"Int64\")\n",
    "\n",
    "        # Mask: rsid coord known AND gene window known AND chr match AND pos within [start,end]\n",
    "        m = has_rsid & has_gene\n",
    "        if m.any():\n",
    "            m = m & (chr_arr == gw_chr) & (pos_arr >= gw_s) & (pos_arr <= gw_e)\n",
    "        return m\n",
    "\n",
    "    with gzip.open(out_path, \"wt\") as gzout:\n",
    "        for chunk in pd.read_csv(path, sep=\"\\t\", compression=\"infer\", chunksize=CHUNKSIZE, dtype=\"string\", usecols=usecols):\n",
    "            total += len(chunk)\n",
    "            m = row_mask(chunk)\n",
    "            sub = chunk.loc[m]\n",
    "            if not sub.empty:\n",
    "                sub.to_csv(gzout, sep=\"\\t\", index=False, header=first)\n",
    "                first = False\n",
    "                written += len(sub)\n",
    "\n",
    "    return {\n",
    "        \"cohort\": name,\n",
    "        \"input_rows\": total,\n",
    "        \"rows_with_mapped_rsid\": matched_rsid,\n",
    "        \"rows_with_mapped_gene\": matched_gene,\n",
    "        \"rows_within_gene_window\": written,\n",
    "        \"outfile\": os.path.basename(out_path),\n",
    "    }\n",
    "\n",
    "overview = []\n",
    "for name, path in cohorts.items():\n",
    "    print(f\"[run] {name}\")\n",
    "    res = process_cohort(name, path)\n",
    "    overview.append(res)\n",
    "    print(f\"  -> kept {res['rows_within_gene_window']:,} / {res['input_rows']:,} rows\")\n",
    "\n",
    "ov = pd.DataFrame(overview)\n",
    "ov_path = os.path.join(out_dir, \"overview.within_gene±{}_{}.csv\".format(UPSTREAM_BP, DOWNSTREAM_BP))\n",
    "ov.to_csv(ov_path, index=False)\n",
    "print(\"\\n[done] Saved overview →\", ov_path)\n",
    "print(\"Outputs in:\", out_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d052815",
   "metadata": {},
   "source": [
    "**Step 4:** Load tabular data (summary stats / annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215dd997-1b08-4972-b604-0e4b865528dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] RSIDs with DHS metadata: 13,010,252\n",
      "[info] Chromosomes with TSS windows: 25\n",
      "[done] Wrote 12,245,788 SNPs → /mnt/f/0.datasets/ens_vsf_dhs_tss50k/rsids_in_DHS_within_TSS50kb.with_dhs.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# SNPs within 500 kb of any TSS, INCLUDING DHS metadata (rsid, chr, pos, dhs_id, dhs_tissue_type)\n",
    "# - Lymphoid precedence when an rsid maps to both lymphoid & myeloid DHS\n",
    "\n",
    "import os, re, glob, gzip\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Paths ----------\n",
    "mapped_dir    = \"/mnt/f/0.datasets/ens_vcf_dhs/\"    # rsid_in_DHS_chr{n}_{lym|mye}.tsv\n",
    "tss_gene_path = \"/mnt/f/10_osteo_MR/datasets/tss_mart_export.txt.gz\"  # Biomart export\n",
    "tss_out_dir   = \"/mnt/f/0.datasets/ens_vsf_dhs_tss50k/\"\n",
    "Path(tss_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Parameters ----------\n",
    "WINDOW = 50_000  # 500 kb\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def merge_intervals(starts: np.ndarray, ends: np.ndarray):\n",
    "    if starts.size == 0:\n",
    "        return starts, ends\n",
    "    order = np.argsort(starts, kind=\"mergesort\")\n",
    "    s = starts[order].astype(np.int64, copy=False)\n",
    "    e = ends[order].astype(np.int64, copy=False)\n",
    "    ms, me = [], []\n",
    "    cs, ce = int(s[0]), int(e[0])\n",
    "    for i in range(1, s.size):\n",
    "        si, ei = int(s[i]), int(e[i])\n",
    "        if si <= ce + 1:\n",
    "            if ei > ce:\n",
    "                ce = ei\n",
    "        else:\n",
    "            ms.append(cs); me.append(ce)\n",
    "            cs, ce = si, ei\n",
    "    ms.append(cs); me.append(ce)\n",
    "    return np.array(ms, dtype=np.int64), np.array(me, dtype=np.int64)\n",
    "\n",
    "def positions_in_intervals(positions, starts, ends):\n",
    "    if positions.size == 0 or starts.size == 0:\n",
    "        return np.zeros(positions.shape[0], dtype=bool)\n",
    "    idx = np.searchsorted(starts, positions, side=\"right\") - 1\n",
    "    idx = np.clip(idx, 0, ends.size - 1)\n",
    "    return positions <= ends[idx]\n",
    "\n",
    "# ---------- 1) Build RSID -> (chr, pos, dhs_id, component) with Lymphoid precedence ----------\n",
    "# Load myeloid first, then override with lymphoid.\n",
    "rsid2info = {}  # rsid -> (chr, pos, identifier, component)\n",
    "chr_tag_re = re.compile(r\"rsid_in_DHS_chr([0-9XYMT]+)_(lym|mye)\\.tsv$\")\n",
    "\n",
    "for priority in (\"mye\", \"lym\"):\n",
    "    for p in glob.glob(os.path.join(mapped_dir, \"rsid_in_DHS_chr*_[lm]ye.tsv\")):\n",
    "        m = chr_tag_re.search(os.path.basename(p))\n",
    "        if not m:\n",
    "            continue\n",
    "        chr_str, tag = m.group(1), m.group(2)\n",
    "        if tag != priority:\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                p, sep=r\"\\s+|,\", engine=\"python\",\n",
    "                usecols=[\"rsid\",\"POS\",\"identifier\",\"component\"],\n",
    "                dtype={\"rsid\":\"string\",\"POS\":\"Int64\",\"identifier\":\"string\",\"component\":\"string\"},\n",
    "            ).dropna(subset=[\"rsid\",\"POS\"])\n",
    "            # If an rsid appears multiple times within the same file, keep the first\n",
    "            df = df.drop_duplicates(subset=[\"rsid\"], keep=\"first\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {p}: {e}\")\n",
    "            continue\n",
    "        for r, pos, ident, comp in zip(df[\"rsid\"].astype(str),\n",
    "                                       df[\"POS\"].astype(int),\n",
    "                                       df[\"identifier\"].astype(\"string\"),\n",
    "                                       df[\"component\"].astype(\"string\")):\n",
    "            if priority == \"lym\" or r not in rsid2info:\n",
    "                rsid2info[r] = (str(chr_str), int(pos), (None if pd.isna(ident) else str(ident)),\n",
    "                                (None if pd.isna(comp) else str(comp)))\n",
    "\n",
    "print(f\"[info] RSIDs with DHS metadata: {len(rsid2info):,}\")\n",
    "\n",
    "# Build per-chromosome aligned arrays for fast filtering\n",
    "chr2arrays = {}\n",
    "for rs, (c, p, ident, comp) in rsid2info.items():\n",
    "    chr2arrays.setdefault(c, {\"rsid\": [], \"pos\": [], \"ident\": [], \"comp\": []})\n",
    "    chr2arrays[c][\"rsid\"].append(rs)\n",
    "    chr2arrays[c][\"pos\"].append(p)\n",
    "    chr2arrays[c][\"ident\"].append(ident)\n",
    "    chr2arrays[c][\"comp\"].append(comp)\n",
    "for c, d in list(chr2arrays.items()):\n",
    "    order = np.argsort(np.array(d[\"pos\"], dtype=np.int64))\n",
    "    chr2arrays[c] = {\n",
    "        \"rsid\":  np.array(d[\"rsid\"], dtype=object)[order],\n",
    "        \"pos\":   np.array(d[\"pos\"], dtype=np.int64)[order],\n",
    "        \"ident\": np.array(d[\"ident\"], dtype=object)[order],\n",
    "        \"comp\":  np.array(d[\"comp\"], dtype=object)[order],\n",
    "    }\n",
    "\n",
    "# ---------- 2) Load TSS and build merged ±500 kb windows per chromosome ----------\n",
    "tss = pd.read_csv(tss_gene_path, sep=\"\\t\", compression=\"infer\", dtype=\"string\")\n",
    "tss = tss.rename(columns={\n",
    "    \"Gene stable ID\": \"gene_id\",\n",
    "    \"Gene name\": \"gene\",\n",
    "    \"Chromosome/scaffold name\": \"chr\",\n",
    "    \"Transcription start site (TSS)\": \"tss\"\n",
    "})\n",
    "tss[\"chr\"] = tss[\"chr\"].astype(str).str.replace(\"^chr\",\"\",regex=True).replace({\"M\":\"MT\"})\n",
    "tss[\"tss\"] = pd.to_numeric(tss[\"tss\"], errors=\"coerce\")\n",
    "tss = tss.dropna(subset=[\"chr\",\"tss\"])\n",
    "tss[\"tss\"] = tss[\"tss\"].astype(np.int64)\n",
    "\n",
    "valid_chr = {str(i) for i in range(1,23)} | {\"X\",\"Y\",\"MT\"}\n",
    "tss = tss[tss[\"chr\"].isin(valid_chr)].copy()\n",
    "\n",
    "chr2intervals = {}\n",
    "for c, sub in tss.groupby(\"chr\", sort=False):\n",
    "    starts = (sub[\"tss\"].to_numpy(np.int64) - WINDOW).clip(min=1)\n",
    "    ends   = sub[\"tss\"].to_numpy(np.int64) + WINDOW\n",
    "    ms, me = merge_intervals(starts, ends)\n",
    "    chr2intervals[str(c)] = (ms, me)\n",
    "print(f\"[info] Chromosomes with TSS windows: {len(chr2intervals)}\")\n",
    "\n",
    "# ---------- 3) Select SNPs within any TSS window and WRITE with DHS metadata ----------\n",
    "rows = []\n",
    "for c, arrays in chr2arrays.items():\n",
    "    if c not in chr2intervals:\n",
    "        continue\n",
    "    s, e = chr2intervals[c]\n",
    "    pos = arrays[\"pos\"]\n",
    "    inside = positions_in_intervals(pos, s, e)\n",
    "    if inside.any():\n",
    "        idx = np.where(inside)[0]\n",
    "        for i in idx:\n",
    "            rows.append((\n",
    "                arrays[\"rsid\"][i],\n",
    "                c,\n",
    "                int(arrays[\"pos\"][i]),\n",
    "                arrays[\"ident\"][i],\n",
    "                arrays[\"comp\"][i],\n",
    "            ))\n",
    "\n",
    "out_path = os.path.join(tss_out_dir, \"rsids_in_DHS_within_TSS50kb.with_dhs.tsv.gz\")\n",
    "with gzip.open(out_path, \"wt\") as gz:\n",
    "    gz.write(\"rsid\\tchr\\tpos\\tdhs_id\\tdhs_tissue_type\\n\")\n",
    "    for r in rows:\n",
    "        # r = (rsid, chr, pos, identifier, component)\n",
    "        gz.write(f\"{r[0]}\\t{r[1]}\\t{r[2]}\\t{'' if r[3] is None else r[3]}\\t{'' if r[4] is None else r[4]}\\n\")\n",
    "\n",
    "print(f\"[done] Wrote {len(rows):,} SNPs → {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b8e9e",
   "metadata": {},
   "source": [
    "**Step 5:** Load tabular data (summary stats / annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b89c38f-946f-4ce6-83ff-2b026dcd7ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] RSIDs in TSS±50kb: 12,245,788\n",
      "[exposure_eqtlgen] kept 185,386 / 487,985 rows → /mnt/f/10_osteo_MR/_results_tables/exposures_in_TSS50kb/exposure_eqtlgen.within_TSS50kb.tsv.gz\n",
      "[exposure_gtex_wb_eqtl] kept 62,492 / 186,199 rows → /mnt/f/10_osteo_MR/_results_tables/exposures_in_TSS50kb/exposure_gtex_wb_eqtl.within_TSS50kb.tsv.gz\n",
      "[exposure_decode_pqtl] kept 147,494 / 675,513 rows → /mnt/f/10_osteo_MR/_results_tables/exposures_in_TSS50kb/exposure_decode_pqtl.within_TSS50kb.tsv.gz\n",
      "[exposure_ukbppp_pqtl] kept 139,707 / 633,569 rows → /mnt/f/10_osteo_MR/_results_tables/exposures_in_TSS50kb/exposure_ukbppp_pqtl.within_TSS50kb.tsv.gz\n",
      "\n",
      "Overview saved → /mnt/f/10_osteo_MR/_results_tables/exposures_in_TSS50kb/overview.filtered_in_TSS50kb.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# Filter exposures to SNPs within 50 kb of any TSS (RSID membership only)\n",
    "\n",
    "import os, gzip\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Inputs ----------\n",
    "tss50k_path = \"/mnt/f/0.datasets/ens_vsf_dhs_tss50k/rsids_in_DHS_within_TSS50kb.with_dhs.tsv.gz\"\n",
    "cohorts = {\n",
    "    \"exposure_eqtlgen\":       \"/mnt/f/10_osteo_MR/MR_ready/exposure_eqtlgen_dhs_index.tsv\",\n",
    "    \"exposure_gtex_wb_eqtl\":  \"/mnt/f/10_osteo_MR/MR_ready/exposure_gtex_whole_blood_eqtl_dhs_index.tsv\",\n",
    "    \"exposure_decode_pqtl\":   \"/mnt/f/10_osteo_MR/MR_ready/exposure_pqtl_decode_MR_dhs.tsv.gz\",\n",
    "    \"exposure_ukbppp_pqtl\":   \"/mnt/f/10_osteo_MR/MR_ready/exposure_ukbppp_pqtl_MR_dhs.tsv.gz\",\n",
    "}\n",
    "\n",
    "# ---------- Output ----------\n",
    "out_dir = \"/mnt/f/10_osteo_MR/_results_tables/exposures_in_TSS50kb\"\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load RSIDs in TSS±50kb ----------\n",
    "rsids_50k = pd.read_csv(\n",
    "    tss50k_path, sep=\"\\t\", compression=\"infer\",\n",
    "    usecols=[\"rsid\"], dtype={\"rsid\":\"string\"}\n",
    ")[\"rsid\"].dropna().astype(str).unique().tolist()\n",
    "rsid_set = set(rsids_50k)\n",
    "print(f\"[info] RSIDs in TSS±50kb: {len(rsid_set):,}\")\n",
    "\n",
    "# ---------- Filter helper ----------\n",
    "def filter_cohort(name: str, in_path: str, rsid_set: set, chunksize: int = 1_000_000):\n",
    "    out_path = os.path.join(out_dir, f\"{name}.within_TSS50kb.tsv.gz\")\n",
    "    total = kept = 0\n",
    "    wrote_header = False\n",
    "\n",
    "    with gzip.open(out_path, \"wt\") as gzout:\n",
    "        for chunk in pd.read_csv(in_path, sep=\"\\t\", compression=\"infer\", dtype=\"string\", chunksize=chunksize):\n",
    "            if \"SNP\" not in chunk.columns:\n",
    "                raise ValueError(f\"{name}: missing 'SNP' column.\")\n",
    "            total += len(chunk)\n",
    "            mask = chunk[\"SNP\"].astype(\"string\").isin(rsid_set)\n",
    "            sub = chunk.loc[mask]\n",
    "            if not sub.empty:\n",
    "                sub.to_csv(gzout, sep=\"\\t\", index=False, header=not wrote_header)\n",
    "                wrote_header = True\n",
    "                kept += len(sub)\n",
    "\n",
    "    print(f\"[{name}] kept {kept:,} / {total:,} rows → {out_path}\")\n",
    "    return {\"cohort\": name, \"input_rows\": total, \"kept_rows\": kept, \"outfile\": os.path.basename(out_path)}\n",
    "\n",
    "# ---------- Run ----------\n",
    "overview = [filter_cohort(n, p, rsid_set) for n, p in cohorts.items()]\n",
    "ov = pd.DataFrame(overview)\n",
    "ov_path = os.path.join(out_dir, \"overview.filtered_in_TSS50kb.csv\")\n",
    "ov.to_csv(ov_path, index=False)\n",
    "print(\"\\nOverview saved →\", ov_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9aaff-c4d3-4421-91c2-7eb023df6c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
